{"name":"Flute","tagline":"A simple tool for running parallel task pipelines for data processing","body":"## Motivation\r\n\r\nThe benefits of a data processing pipeline are:\r\n\r\n- Being able to easily feed in inputs and running tasks in a data processing environment that otherwise you would have to type in the command line everytime. \r\n- Forcing data analysts to write what they run on the command line in a configuration file so that others (or themselves) will be able to easily follow, remember and redo at a later time. \r\n- Keep track of everything that happened to a data input and be able to easily rerun it all if needed.\r\n\r\nThere are a lot of tools out there for running data processing pipelines. However I couldn't find a lightweight tool with the following features:\r\n\r\n- Would not make you change the code you already have\r\n- Support any language/executable by supporting files \r\n- Providing the ability to run a task in parallel for its inputs\r\n- Live pipeline manipulation\r\n\r\nNote: Flute is currently in a very early stage (a very simple script really). A lot of decent ideas can be added to it. If you thought of a feature, feel free to contribute to the code or add the feature request in the [issues](https://github.com/auxiliary/flute/issues). \r\n\r\n## Installation\r\n\r\n`sudo python setup.py install` or if you want to install locally: `python setup.py install --user`\r\n\r\n## Usage\r\n\r\nCreate a configuration file called `config.ini` and write the configuration of your pipeline there. \r\nThen run `flute <pipeline name>` to run the pipeline. \r\n\r\nThe configuration file may contain several tasks, pipelines and comments with in the `ini` format. Here's an example:\r\n\r\n```ini\r\n[task sample_task]\r\nfile = somefile.py\r\ninput = input1.csv input2.csv input3.csv\r\nparallel = true\r\n\r\n[task sample_task2]\r\nfile = anotherfile.sh\r\ninput = stdin\r\n\r\n[pipeline pipe1]\r\ntasks = sample_task sample_task2\r\n```\r\n\r\nThis pipeline (`pipe1`) runs the first task (`sample_task`) first and then runs sample_task2. For the first task, 3 copies of the file `somefile.py` are executed with the inputs of input1.csv, input2.csv and input3.csv respectively. The output of this task is then fed into sample_task2 that's run serially. The keyword `stdin` tells Flute to take the input from the pipe (meaning from the first task in this case). \r\n\r\nFor running this sample pipeline, you would run:\r\n\r\n`flute pipe1`\r\n\r\nVariables can be used in the configuration files as they're normal ini files. So for example, we can have:\r\n\r\n```ini\r\n[task task1]\r\nfile = variable_extraction.py\r\ninput = input_file.nc input_file2.nc\r\n\r\n[task task2]\r\nfile = quantization.py\r\ninput = ${task task1:input}\r\n```\r\n\r\nYou can list the tasks, pipelines and other configurations with the `--list` argument. \r\n\r\n## Environment variables as external configurations\r\n\r\nIn almost every situation, you might need a set of configuration variables that are common between your data processing scripts. Also, you might write your scripts in different languages, like Shell, Python or C, etc. The easiest way to share common configuration variables between these scripts while also minimizing your dependency to a tool like Flute is via **Environment Variables**. \r\n\r\nThis is why Flute automatically takes all variables under the **config** section of your configuration file and sets them as environment variables for your scripts to use. For example, in the following configuration, `WORKSPACE` and `FILL_VALUE` will be set as environment variables.\r\n\r\n```ini\r\n[config]\r\nWORKSPACE=~/project/example_path\r\nFILL_VALUE=1e+20\r\n\r\n...\r\n```\r\n\r\n## Dynamic live manipulation of the pipeline (ZMQ console)\r\n\r\nIf you need to change the pipeline on the fly as it's running, flute provides some options using ZMQ. Using `flute -c` or `flute --console` you can launch a console that connects to the running pipeline run. Currently, only the following commands can be executed on the console:\r\n\r\n- **inject <task name>** Injects a task to the running pipeline (the task will be injected as the next task)\r\n- **reload** Reloads the configuration file in case you have added a new task\r\n- **skip** Skips the next task on the pipeline\r\n- **end** End the pipeline after the current running task\r\n- **resume** resumes a broken pipeline\r\n- **q/quit** quits the console\r\n\r\n## Fault tolerance\r\n\r\nCurrently, if a task fails instead of breaking the pipeline, it will pause it and wait until you resume it with the `resume` command in the console. This gives users the opportunity to fix something, alter the pipeline and resume their work instead of just breaking everything. \r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}